{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTS"
      ],
      "metadata": {
        "id": "R7kRtpMn8-_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk pyspark\n",
        "import os\n",
        "import zipfile\n",
        "import time\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import FloatType, ArrayType, StringType, IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, MinHashLSH, HashingTF\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQIvbfNI7La_",
        "outputId": "2d4833a5-b03f-4265-c6be-d86f847fb481"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SETUP AND DATA LOADING"
      ],
      "metadata": {
        "id": "YxvcQl5n9HNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaggle credentials setup\n",
        "os.environ['KAGGLE_USERNAME'] = 'fabiocasalingo'\n",
        "os.environ['KAGGLE_KEY'] = 'f634e9d85346043bccbcb39b5cd6917e'\n",
        "\n",
        "# Create directory for dataset\n",
        "dataset_dir = \"./kaggle_data\"\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# File paths\n",
        "file_name = \"Books_rating.csv\"\n",
        "zip_path = os.path.join(dataset_dir, \"amazon-books-reviews.zip\")\n",
        "\n",
        "# Download dataset\n",
        "print(\"Download in corso...\")\n",
        "get_ipython().system('kaggle datasets download -d mohamedbakhet/amazon-books-reviews -p {dataset_dir} --force')\n",
        "print(\"Download completato.\")\n",
        "\n",
        "file_path = os.path.join(dataset_dir, \"Books_rating.csv\")\n",
        "print(f\"File CSV utilizzato: {file_path}\")\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_dir)\n",
        "    print(\"Estrazione completata.\")\n",
        "\n",
        "# Configuration parameters\n",
        "SAMPLE_SIZE = 10000\n",
        "LSH_HASH_TABLES = 3\n",
        "VOCAB_SIZE = 5000\n",
        "JACCARD_THRESHOLD = 0.2\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "print(f\"Using optimized configuration:\")\n",
        "print(f\"- Sample size: {SAMPLE_SIZE}\")\n",
        "print(f\"- LSH hash tables: {LSH_HASH_TABLES}\")\n",
        "print(f\"- Vocabulary size: {VOCAB_SIZE}\")\n",
        "print(f\"- Jaccard threshold: {JACCARD_THRESHOLD}\")\n",
        "\n",
        "# Spark session initialization\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BookRatingAnalysisWithLSH\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(\"SparkSession creata con successo!\")\n",
        "\n",
        "# Load the dataset\n",
        "df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(file_path)\n",
        "\n",
        "print(\"Dataset caricato correttamente.\")\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq82EIPS7Mc_",
        "outputId": "528b8a93-2386-46ac-8ca7-8f3f6220f366"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download in corso...\n",
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to ./kaggle_data\n",
            " 99% 1.05G/1.06G [00:17<00:00, 125MB/s]\n",
            "100% 1.06G/1.06G [00:17<00:00, 64.1MB/s]\n",
            "Download completato.\n",
            "File CSV utilizzato: ./kaggle_data/Books_rating.csv\n",
            "Estrazione completata.\n",
            "Using optimized configuration:\n",
            "- Sample size: 10000\n",
            "- LSH hash tables: 3\n",
            "- Vocabulary size: 5000\n",
            "- Jaccard threshold: 0.2\n",
            "Spark version: 3.5.1\n",
            "SparkSession creata con successo!\n",
            "Dataset caricato correttamente.\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|1882931173|Its Only Art If I...| NULL| AVCGYZL8FQQTD|\"Jim of Oz \"\"jim-...|               7/7|         4.0|  940636800|Nice collection o...|This is only for ...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A30TK6U7DNS82R|       Kevin Killian|             10/10|         5.0| 1095724800|   Really Enjoyed It|I don't care much...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A3UH4UZ4RSVO82|        John Granger|             10/11|         5.0| 1078790400|Essential for eve...|\"If people become...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A2MVUWT453QH61|\"Roy E. Perry \"\"a...|               7/7|         4.0| 1090713600|Phlip Nel gives s...|Theodore Seuss Ge...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A22X4XUPKF66MR|\"D. H. Richards \"...|               3/3|         4.0| 1107993600|Good academic ove...|\"Philip Nel - Dr....|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "HpDYIjba9PaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter empty or short reviews\n",
        "df_clean = df.filter(\n",
        "    (F.col(\"review/text\").isNotNull()) &\n",
        "    (F.col(\"review/text\") != \"\") &\n",
        "    (F.length(F.col(\"review/text\")) >= 10)\n",
        ").select(\n",
        "    F.monotonically_increasing_id().alias(\"review_id\"),\n",
        "    F.col(\"review/text\").alias(\"review_text\")\n",
        ").limit(SAMPLE_SIZE)\n",
        "\n",
        "print(f\"Clean dataset: {df_clean.count()} reviews\")\n",
        "\n",
        "# Install and import NLTK for stemming\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def stem_words(word_list):\n",
        "    return [stemmer.stem(word) for word in word_list if len(word) > 2]\n",
        "\n",
        "stem_udf = F.udf(stem_words, ArrayType(StringType()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGIUnxpE9KRa",
        "outputId": "2509002b-e696-4e42-bc46-e4fe62c04677"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean dataset: 10000 reviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT PROCESSING PIPELINE"
      ],
      "metadata": {
        "id": "vc7d0ePQIrWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing text...\")\n",
        "\n",
        "# Clean and normalize text\n",
        "df_processed = df_clean.withColumn(\n",
        "    \"review_text_clean\",\n",
        "    F.regexp_replace(F.lower(F.col(\"review_text\")), \"[^a-z\\\\s]\", \" \")\n",
        ").withColumn(\n",
        "    \"review_text_clean\",\n",
        "    F.regexp_replace(F.col(\"review_text_clean\"), \"\\\\s+\", \" \")\n",
        ")\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = RegexTokenizer(\n",
        "    inputCol=\"review_text_clean\",\n",
        "    outputCol=\"words\",\n",
        "    pattern=\"\\\\s+\",\n",
        "    gaps=True\n",
        ")\n",
        "df_tokenized = tokenizer.transform(df_processed)\n",
        "\n",
        "# Remove stopwords\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "df_filtered = remover.transform(df_tokenized)\n",
        "\n",
        "# Remove short words\n",
        "df_filtered = df_filtered.withColumn(\n",
        "    \"filtered_words\",\n",
        "    F.expr(\"filter(filtered_words, x -> length(x) > 2)\")\n",
        ")\n",
        "\n",
        "# Apply stemming\n",
        "df_stemmed = df_filtered.withColumn(\n",
        "    \"stemmed_words\",\n",
        "    stem_udf(\"filtered_words\")\n",
        ")\n",
        "\n",
        "# Remove duplicate words\n",
        "df_final = df_stemmed.withColumn(\n",
        "    \"unique_words\",\n",
        "    F.array_distinct(F.col(\"stemmed_words\"))\n",
        ").withColumn(\n",
        "    \"num_words\",\n",
        "    F.size(\"unique_words\")\n",
        ").filter(\n",
        "    F.col(\"num_words\") >= 3\n",
        ")\n",
        "\n",
        "df_final.cache()\n",
        "print(f\"Final dataset: {df_final.count()} reviews\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcP5Mt61IwgU",
        "outputId": "c8b9b84a-4d78-48ed-f920-72efcb9d4c82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing text...\n",
            "Final dataset: 9949 reviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE ENGINEERING WITH HASHINGTF"
      ],
      "metadata": {
        "id": "KLS6h35uIzcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating optimized feature vectors...\")\n",
        "\n",
        "hashingTF = HashingTF(\n",
        "    inputCol=\"unique_words\",\n",
        "    outputCol=\"features\",\n",
        "    numFeatures=VOCAB_SIZE\n",
        ")\n",
        "\n",
        "df_with_features = hashingTF.transform(df_final)\n",
        "\n",
        "# Remove empty vectors\n",
        "nnz_udf = F.udf(lambda v: len(v.indices), IntegerType())\n",
        "\n",
        "df_with_features = df_with_features.withColumn(\"nnz\", nnz_udf(\"features\")) \\\n",
        "    .filter(F.col(\"nnz\") > 0) \\\n",
        "    .drop(\"nnz\") \\\n",
        "    .cache()\n",
        "\n",
        "print(\"Feature vectors created successfully!\")\n",
        "print(f\"Reviews with non-zero features: {df_with_features.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8FObUOQJJ-1",
        "outputId": "8da07ff6-390f-4659-a1bb-6cbd40927634"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating optimized feature vectors...\n",
            "Feature vectors created successfully!\n",
            "Reviews with non-zero features: 9949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATE ARTIFICIAL DUPLICATES"
      ],
      "metadata": {
        "id": "dIOEjhPGJML4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'='*60}\")\n",
        "print(\"CREATING ARTIFICIAL DUPLICATES FOR EVALUATION\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "num_duplicates = 25\n",
        "dup_source = df_final.orderBy(F.rand(seed=RANDOM_SEED)).limit(num_duplicates)\n",
        "\n",
        "duplicated = dup_source.withColumn(\n",
        "    \"review_id\",\n",
        "    F.concat(F.lit(\"dup_\"), F.col(\"review_id\").cast(\"string\"))\n",
        ")\n",
        "\n",
        "df_augmented = df_final.unionByName(duplicated)\n",
        "print(f\"Augmented dataset size: {df_augmented.count()}\")\n",
        "print(f\"Added {num_duplicates} artificial duplicates\")\n",
        "\n",
        "df_aug_with_features = hashingTF.transform(df_augmented)\n",
        "\n",
        "df_aug_with_features = df_aug_with_features.withColumn(\"nnz\", nnz_udf(\"features\")) \\\n",
        "    .filter(F.col(\"nnz\") > 0) \\\n",
        "    .drop(\"nnz\") \\\n",
        "    .cache()\n",
        "\n",
        "print(f\"Augmented reviews with non-zero features: {df_aug_with_features.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaTR0NFhJPMK",
        "outputId": "d18e199e-7b72-4bba-84a3-7ab2585aaa6d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CREATING ARTIFICIAL DUPLICATES FOR EVALUATION\n",
            "============================================================\n",
            "Augmented dataset size: 9974\n",
            "Added 25 artificial duplicates\n",
            "Augmented reviews with non-zero features: 9974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSH SIMILARITY DETECTION FUNCTION"
      ],
      "metadata": {
        "id": "J19wTAP2JSB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimized_lsh_similarity_with_evaluation(df, df_augmented, threshold=0.5):\n",
        "    print(f\"Running LSH evaluation (threshold: {threshold})...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    mh = MinHashLSH(\n",
        "        inputCol=\"features\",\n",
        "        outputCol=\"hashes\",\n",
        "        numHashTables=LSH_HASH_TABLES,\n",
        "        seed=RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    print(\"Fitting LSH model on augmented data...\")\n",
        "    model = mh.fit(df_augmented)\n",
        "\n",
        "    df_hashed = model.transform(df_augmented.select(\"review_id\", \"features\"))\n",
        "\n",
        "    print(\"Finding similar pairs...\")\n",
        "    distance_threshold = 1.0 - threshold\n",
        "\n",
        "    similar_pairs = model.approxSimilarityJoin(\n",
        "        df_hashed,\n",
        "        df_hashed,\n",
        "        distance_threshold,\n",
        "        distCol=\"distance\"\n",
        "    ).filter(\n",
        "        F.col(\"datasetA.review_id\") < F.col(\"datasetB.review_id\")\n",
        "    ).withColumn(\n",
        "        \"similarity\",\n",
        "        1.0 - F.col(\"distance\")\n",
        "    ).select(\n",
        "        F.col(\"datasetA.review_id\").alias(\"review_id_1\"),\n",
        "        F.col(\"datasetB.review_id\").alias(\"review_id_2\"),\n",
        "        F.col(\"similarity\").alias(\"lsh_score\")\n",
        "    ).cache()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    result_count = similar_pairs.count()\n",
        "\n",
        "    print(f\"LSH completed in {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Found {result_count} similar pairs\")\n",
        "\n",
        "    return similar_pairs, model"
      ],
      "metadata": {
        "id": "EJect_NDJVxJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METRIC CALCULATION FUNCTION"
      ],
      "metadata": {
        "id": "iaL2apBZJYHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_evaluation_metrics(predicted_pairs, num_duplicates):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"CALCULATING EVALUATION METRICS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    pred_pairs_for_eval = predicted_pairs.select(\"review_id_1\", \"review_id_2\")\n",
        "\n",
        "    true_positives = pred_pairs_for_eval.filter(\n",
        "        F.col(\"review_id_2\").startswith(\"dup_\") &\n",
        "        (F.col(\"review_id_2\") == F.concat(F.lit(\"dup_\"), F.col(\"review_id_1\")))\n",
        "    ).count()\n",
        "\n",
        "    total_predictions = pred_pairs_for_eval.count()\n",
        "    total_ground_truth = num_duplicates\n",
        "\n",
        "    precision = true_positives / total_predictions if total_predictions > 0 else 0.0\n",
        "    recall = true_positives / total_ground_truth if total_ground_truth > 0 else 0.0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    print(f\"Evaluation Results:\")\n",
        "    print(f\"{'─' * 40}\")\n",
        "    print(f\"Ground truth duplicate pairs  : {total_ground_truth}\")\n",
        "    print(f\"Total predicted pairs         : {total_predictions}\")\n",
        "    print(f\"True positives (TP)          : {true_positives}\")\n",
        "    print(f\"Precision                    : {precision:.3f}\")\n",
        "    print(f\"Recall                       : {recall:.3f}\")\n",
        "    print(f\"F1-Score                     : {f1_score:.3f}\")\n",
        "\n",
        "    if true_positives > 0:\n",
        "        print(f\"\\nExample True Positive Pairs:\")\n",
        "        example_tp = pred_pairs_for_eval.filter(\n",
        "            F.col(\"review_id_2\").startswith(\"dup_\") &\n",
        "            (F.col(\"review_id_2\") == F.concat(F.lit(\"dup_\"), F.col(\"review_id_1\")))\n",
        "        ).limit(5)\n",
        "        example_tp.show(truncate=False)\n",
        "\n",
        "    return {\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score,\n",
        "        'true_positives': true_positives,\n",
        "        'total_predictions': total_predictions,\n",
        "        'total_ground_truth': total_ground_truth\n",
        "    }"
      ],
      "metadata": {
        "id": "c3UwfydxJbtL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXECUTE SIMILARITY DETECTION"
      ],
      "metadata": {
        "id": "NPMjYbJPJeXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n{'='*60}\")\n",
        "print(\"OPTIMIZED SIMILARITY DETECTION WITH EVALUATION\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "threshold = 0.5\n",
        "print(f\"\\nTesting threshold: {threshold}\")\n",
        "print(f\"{'-'*40}\")\n",
        "\n",
        "lsh_results, lsh_model = optimized_lsh_similarity_with_evaluation(\n",
        "    df_with_features, df_aug_with_features, threshold\n",
        ")\n",
        "\n",
        "metrics = calculate_evaluation_metrics(lsh_results, num_duplicates)\n",
        "\n",
        "if lsh_results.count() > 0:\n",
        "    print(f\"\\nTop 10 Similar Pairs Found:\")\n",
        "    lsh_results.orderBy(F.col(\"lsh_score\").desc()).show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvtFvZtIJhgZ",
        "outputId": "806ddc59-2f56-4bb3-c4f8-d6b310c1e225"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "OPTIMIZED SIMILARITY DETECTION WITH EVALUATION\n",
            "============================================================\n",
            "\n",
            "Testing threshold: 0.5\n",
            "----------------------------------------\n",
            "Running LSH evaluation (threshold: 0.5)...\n",
            "Fitting LSH model on augmented data...\n",
            "Finding similar pairs...\n",
            "LSH completed in 2.02 seconds\n",
            "Found 272 similar pairs\n",
            "\n",
            "============================================================\n",
            "CALCULATING EVALUATION METRICS\n",
            "============================================================\n",
            "Evaluation Results:\n",
            "────────────────────────────────────────\n",
            "Ground truth duplicate pairs  : 25\n",
            "Total predicted pairs         : 272\n",
            "True positives (TP)          : 25\n",
            "Precision                    : 0.092\n",
            "Recall                       : 1.000\n",
            "F1-Score                     : 0.168\n",
            "\n",
            "Example True Positive Pairs:\n",
            "+-----------+-----------+\n",
            "|review_id_1|review_id_2|\n",
            "+-----------+-----------+\n",
            "|9900       |dup_9900   |\n",
            "|5444       |dup_5444   |\n",
            "|2567       |dup_2567   |\n",
            "|4425       |dup_4425   |\n",
            "|7317       |dup_7317   |\n",
            "+-----------+-----------+\n",
            "\n",
            "\n",
            "Top 10 Similar Pairs Found:\n",
            "+-----------+-----------+---------+\n",
            "|review_id_1|review_id_2|lsh_score|\n",
            "+-----------+-----------+---------+\n",
            "|8998       |9093       |1.0      |\n",
            "|8099       |9998       |1.0      |\n",
            "|9093       |9210       |1.0      |\n",
            "|422        |423        |1.0      |\n",
            "|6314       |6465       |1.0      |\n",
            "|7660       |dup_7660   |1.0      |\n",
            "|1758       |2475       |1.0      |\n",
            "|2819       |2824       |1.0      |\n",
            "|724        |790        |1.0      |\n",
            "|5380       |5396       |1.0      |\n",
            "+-----------+-----------+---------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THRESHOLD ANALYSIS"
      ],
      "metadata": {
        "id": "alpoNmavJksl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_different_thresholds(df_features, df_aug_features, thresholds=[0.3, 0.5, 0.7, 0.8]):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"THRESHOLD ANALYSIS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for thresh in thresholds:\n",
        "        print(f\"\\nAnalyzing threshold: {thresh}\")\n",
        "        pairs, _ = optimized_lsh_similarity_with_evaluation(df_features, df_aug_features, thresh)\n",
        "        metrics = calculate_evaluation_metrics(pairs, num_duplicates)\n",
        "        metrics['threshold'] = thresh\n",
        "        results.append(metrics)\n",
        "        print(f\"Threshold {thresh}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(f\"\\nRunning threshold analysis...\")\n",
        "threshold_results = analyze_different_thresholds(\n",
        "    df_with_features,\n",
        "    df_aug_with_features,\n",
        "    thresholds=[0.3, 0.5, 0.7]\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"THRESHOLD ANALYSIS SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"{'Threshold':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Pairs':<10}\")\n",
        "print(f\"{'-'*50}\")\n",
        "for result in threshold_results:\n",
        "    print(f\"{result['threshold']:<10} {result['precision']:<10.3f} {result['recall']:<10.3f} \"\n",
        "          f\"{result['f1_score']:<10.3f} {result['total_predictions']:<10}\")\n",
        "\n",
        "best_result = max(threshold_results, key=lambda x: x['f1_score'])\n",
        "print(f\"\\nBest threshold: {best_result['threshold']} (F1-Score: {best_result['f1_score']:.3f})\")\n",
        "\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5w2AZARpJnS2",
        "outputId": "892fe0d2-adeb-4c7c-cce6-1812f602c098"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running threshold analysis...\n",
            "\n",
            "============================================================\n",
            "THRESHOLD ANALYSIS\n",
            "============================================================\n",
            "\n",
            "Analyzing threshold: 0.3\n",
            "Running LSH evaluation (threshold: 0.3)...\n",
            "Fitting LSH model on augmented data...\n",
            "Finding similar pairs...\n",
            "LSH completed in 0.59 seconds\n",
            "Found 520 similar pairs\n",
            "\n",
            "============================================================\n",
            "CALCULATING EVALUATION METRICS\n",
            "============================================================\n",
            "Evaluation Results:\n",
            "────────────────────────────────────────\n",
            "Ground truth duplicate pairs  : 25\n",
            "Total predicted pairs         : 520\n",
            "True positives (TP)          : 25\n",
            "Precision                    : 0.048\n",
            "Recall                       : 1.000\n",
            "F1-Score                     : 0.092\n",
            "\n",
            "Example True Positive Pairs:\n",
            "+-----------+-----------+\n",
            "|review_id_1|review_id_2|\n",
            "+-----------+-----------+\n",
            "|9900       |dup_9900   |\n",
            "|5444       |dup_5444   |\n",
            "|2567       |dup_2567   |\n",
            "|4425       |dup_4425   |\n",
            "|7317       |dup_7317   |\n",
            "+-----------+-----------+\n",
            "\n",
            "Threshold 0.3: P=0.048, R=1.000, F1=0.092\n",
            "\n",
            "Analyzing threshold: 0.5\n",
            "Running LSH evaluation (threshold: 0.5)...\n",
            "Fitting LSH model on augmented data...\n",
            "Finding similar pairs...\n",
            "LSH completed in 1.16 seconds\n",
            "Found 272 similar pairs\n",
            "\n",
            "============================================================\n",
            "CALCULATING EVALUATION METRICS\n",
            "============================================================\n",
            "Evaluation Results:\n",
            "────────────────────────────────────────\n",
            "Ground truth duplicate pairs  : 25\n",
            "Total predicted pairs         : 272\n",
            "True positives (TP)          : 25\n",
            "Precision                    : 0.092\n",
            "Recall                       : 1.000\n",
            "F1-Score                     : 0.168\n",
            "\n",
            "Example True Positive Pairs:\n",
            "+-----------+-----------+\n",
            "|review_id_1|review_id_2|\n",
            "+-----------+-----------+\n",
            "|9900       |dup_9900   |\n",
            "|5444       |dup_5444   |\n",
            "|2567       |dup_2567   |\n",
            "|4425       |dup_4425   |\n",
            "|7317       |dup_7317   |\n",
            "+-----------+-----------+\n",
            "\n",
            "Threshold 0.5: P=0.092, R=1.000, F1=0.168\n",
            "\n",
            "Analyzing threshold: 0.7\n",
            "Running LSH evaluation (threshold: 0.7)...\n",
            "Fitting LSH model on augmented data...\n",
            "Finding similar pairs...\n",
            "LSH completed in 0.58 seconds\n",
            "Found 255 similar pairs\n",
            "\n",
            "============================================================\n",
            "CALCULATING EVALUATION METRICS\n",
            "============================================================\n",
            "Evaluation Results:\n",
            "────────────────────────────────────────\n",
            "Ground truth duplicate pairs  : 25\n",
            "Total predicted pairs         : 255\n",
            "True positives (TP)          : 25\n",
            "Precision                    : 0.098\n",
            "Recall                       : 1.000\n",
            "F1-Score                     : 0.179\n",
            "\n",
            "Example True Positive Pairs:\n",
            "+-----------+-----------+\n",
            "|review_id_1|review_id_2|\n",
            "+-----------+-----------+\n",
            "|9900       |dup_9900   |\n",
            "|5444       |dup_5444   |\n",
            "|2567       |dup_2567   |\n",
            "|4425       |dup_4425   |\n",
            "|7317       |dup_7317   |\n",
            "+-----------+-----------+\n",
            "\n",
            "Threshold 0.7: P=0.098, R=1.000, F1=0.179\n",
            "\n",
            "============================================================\n",
            "THRESHOLD ANALYSIS SUMMARY\n",
            "============================================================\n",
            "Threshold  Precision  Recall     F1-Score   Pairs     \n",
            "--------------------------------------------------\n",
            "0.3        0.048      1.000      0.092      520       \n",
            "0.5        0.092      1.000      0.168      272       \n",
            "0.7        0.098      1.000      0.179      255       \n",
            "\n",
            "Best threshold: 0.7 (F1-Score: 0.179)\n"
          ]
        }
      ]
    }
  ]
}